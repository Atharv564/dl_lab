{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2416a6f2-14df-4619-ad3f-b45106603078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Data Preparation \n",
    "import numpy as np \n",
    "from collections import defaultdict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb1ed33c-9171-4922-8e42-7670c535e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus \n",
    "corpus = [ \n",
    "    \"the quick brown fox jumped over the lazy dog\", \n",
    "    \"I love playing with my dog\", \n",
    "    \"the dog is quick and smart\" \n",
    "] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d77729-db1e-43b9-925d-b92d7f9f9148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'the': 0, 'fox': 1, 'over': 2, 'i': 3, 'playing': 4, 'my': 5, 'quick': 6, 'and': 7, 'love': 8, 'with': 9, 'is': 10, 'smart': 11, 'brown': 12, 'jumped': 13, 'dog': 14, 'lazy': 15}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize \n",
    "words = [] \n",
    "for sentence in corpus: \n",
    "    for word in sentence.lower().split(): \n",
    "        words.append(word) \n",
    "vocab = set(words) \n",
    "word2idx = {w: idx for idx, w in enumerate(vocab)} \n",
    "idx2word = {idx: w for w, idx in word2idx.items()} \n",
    "vocab_size = len(vocab) \n",
    "print(\"Vocabulary:\", word2idx) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f870301d-4995-4408-86c2-a10a251da552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Generate training data (CBOW: context -> center word) \n",
    "window_size = 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e28bc92e-595b-4266-b07f-485fabbbcd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(words, window_size):\n",
    "    data = []\n",
    "    for i in range(window_size, len(words) - window_size):\n",
    "        context = []\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if j != 0:\n",
    "                context.append(words[i + j])\n",
    "        target = words[i]\n",
    "        data.append((context, target))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23d59e3f-4044-46d3-be7c-3da1c13bb86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample training data (context -> target):\n",
      "['the', 'quick', 'fox', 'jumped'] -> brown\n",
      "['quick', 'brown', 'jumped', 'over'] -> fox\n",
      "['brown', 'fox', 'over', 'the'] -> jumped\n",
      "['fox', 'jumped', 'the', 'lazy'] -> over\n",
      "['jumped', 'over', 'lazy', 'dog'] -> the\n"
     ]
    }
   ],
   "source": [
    "training_data = generate_training_data(words, window_size)\n",
    "print(\"\\nSample training data (context -> target):\") \n",
    "for context, target in training_data[:5]: \n",
    "    print(context, \"->\", target) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "816510e0-d173-4d87-aee8-bfbebb3d814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding \n",
    "def one_hot_vector(word): \n",
    "    vec = np.zeros(vocab_size) \n",
    "    vec[word2idx[word]] = 1 \n",
    "    return vec \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba9bb4f1-fa69-40aa-ab2e-6ab4acf2de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training sets \n",
    "X_train = [] \n",
    "y_train = [] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2bca082-9b0e-49da-bc1f-c1c40b1898d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for context, target in training_data:\n",
    "    context_vec = np.zeros(vocab_size)\n",
    "    for w in context:\n",
    "        context_vec += one_hot_vector(w)  \n",
    "    X_train.append(context_vec)\n",
    "    y_train.append(one_hot_vector(target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "946123dd-3cce-4152-8621-8f7104869316",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d79701e3-edbe-4bfd-8ae9-0b5731f4e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Train Model (CBOW using simple neural network) \n",
    "embedding_dim = 10 # size of hidden layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e2b0edd-be5f-4484-acb4-4bf173225153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights \n",
    "W1 = np.random.randn(vocab_size, embedding_dim) \n",
    "W2 = np.random.randn(embedding_dim, vocab_size) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18cc5d88-3270-478f-b765-4a207e12b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters \n",
    "lr = 0.01 \n",
    "epochs = 2000 \n",
    "\n",
    "def softmax(x): \n",
    "    e_x = np.exp(x - np.max(x)) \n",
    "    return e_x / e_x.sum(axis=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e27d936-d6ce-410d-8066-2a3687e6b504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 142.0672\n",
      "Epoch 200, Loss: 0.5007\n",
      "Epoch 400, Loss: 0.2176\n",
      "Epoch 600, Loss: 0.1356\n",
      "Epoch 800, Loss: 0.0974\n",
      "Epoch 1000, Loss: 0.0756\n",
      "Epoch 1200, Loss: 0.0615\n",
      "Epoch 1400, Loss: 0.0516\n",
      "Epoch 1600, Loss: 0.0444\n",
      "Epoch 1800, Loss: 0.0389\n",
      "\n",
      "Word embeddings (rows = words):\n",
      "the : [-0.38863192 -0.95496354  0.25487471 -0.69653746  2.04599293  0.61005768\n",
      " -2.24678005 -0.72163723  1.35788491  0.52597301]\n",
      "fox : [ 0.37073786  1.44496748 -0.29078907  0.41768502  0.79790655 -1.5149945\n",
      " -0.04489068 -0.50071754 -1.75035752  2.4596813 ]\n",
      "over : [-0.01709991 -0.31965164 -0.85496474  0.4451881  -0.96375938  0.06823534\n",
      "  1.70847518  0.66545708  1.76049585  1.37519817]\n",
      "i : [ 1.84515532 -1.47099442 -0.93772927 -0.97575407 -0.58789594  0.70305567\n",
      " -0.75233154  0.86052084  0.53661963  0.31534405]\n",
      "playing : [ 0.41699327 -0.3338303   0.20126558  1.96154822 -0.11252112 -1.29557228\n",
      "  2.27922366 -0.75395952  0.19021353 -0.47387182]\n",
      "my : [-1.41612498 -0.08617071 -0.38318465  0.03567377  1.2147519  -0.2328266\n",
      " -1.87043739  3.08563294 -0.25100459 -0.36040667]\n",
      "quick : [ 0.48821837 -0.26508547 -0.00613552 -1.00625961  1.66507577 -0.2404962\n",
      " -1.34931652 -1.24195379 -0.79399046 -0.71951606]\n",
      "and : [ 1.1262879  -1.17559074 -0.76666641 -0.1205876   0.1911056   2.4330404\n",
      " -0.14549853 -0.62206209  0.08389143 -1.48019084]\n",
      "love : [ 0.07968642 -1.68581874 -0.45871906 -1.34620295  1.29339736 -0.27963069\n",
      " -1.04474069  0.34557923 -0.61623823  1.22434498]\n",
      "with : [-0.03925556 -0.71064978  0.13415665 -0.06450037  0.32028553  0.25661918\n",
      "  0.11658051 -0.88566018 -0.67247946 -1.06483614]\n",
      "is : [-0.62784833  1.58567191  0.35822191  1.22409116  0.71342123 -0.32237249\n",
      " -2.29657658  0.45287896 -0.73786361 -0.69372066]\n",
      "smart : [-1.16809277  0.12964178 -0.20784253 -1.36302552  0.42930212 -0.34369119\n",
      "  1.01531342  0.04174963 -1.08555379 -1.00728086]\n",
      "brown : [ 0.33053847  1.47462366 -1.34881483 -0.74815311 -0.70636552  1.46037681\n",
      " -0.49682467 -0.98017475  0.93669804  0.49422059]\n",
      "jumped : [-0.38969062  1.0749455   0.01710739  1.58907416 -0.83761186  0.67191804\n",
      "  0.69507287  0.50957565  0.14492458 -1.58757927]\n",
      "dog : [ 0.82579482  0.9400943  -1.04378926 -0.23960036 -1.15791658 -1.20743824\n",
      "  0.48314899  0.37767844  0.55151458 -2.25751141]\n",
      "lazy : [-0.062436    1.01409381  0.81332385  0.42817248 -0.42428249 -1.93581079\n",
      "  0.52370643 -0.00955609  1.07680417  0.7456026 ]\n"
     ]
    }
   ],
   "source": [
    "# Training loop \n",
    "for epoch in range(epochs): \n",
    "    loss = 0 \n",
    "    for x, y in zip(X_train, y_train):\n",
    "        \n",
    "        # Forward pass \n",
    "        h = np.dot(x, W1) # hidden layer \n",
    "        u = np.dot(h, W2) # output scores \n",
    "        y_pred = softmax(u) # prediction \n",
    "           \n",
    "        # Loss (cross-entropy) \n",
    "        loss -= np.sum(y * np.log(y_pred + 1e-9)) \n",
    "           \n",
    "        # Backpropagation \n",
    "        e = y_pred - y\n",
    "        dW2 = np.outer(h, e) \n",
    "        dW1 = np.outer(x, np.dot(W2, e)) \n",
    "           \n",
    "        # Update weights \n",
    "        W1 -= lr * dW1 \n",
    "        W2 -= lr * dW2 \n",
    "        \n",
    "    if epoch % 200 == 0: \n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\") \n",
    "        \n",
    "# d. Output: Word embeddings \n",
    "print(\"\\nWord embeddings (rows = words):\") \n",
    "for word in word2idx: \n",
    "    print(word, \":\", W1[word2idx[word]]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f0ea0-78f4-490e-9e77-fbe4d654b123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
